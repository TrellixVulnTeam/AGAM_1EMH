# Attributes-Guided and Pure-Visual Attention Alignment for Few-Shot Recognition

**The code will be available soon!**

This project is the PyTorch implementation of the paper "[Attributes-Guided and Pure-Visual Attention Alignment for Few-Shot Recognition]()", in which we propose an attributes-guided attention module (AGAM) to utilize human-annotated attributes and learn more discriminative features for few-shot recognition. The network architecture is illustrated in the following figure, and more details can be found in the paper.

![](https://raw.githubusercontent.com/bighuang624/AGAM/master/docs/AGAM-model-structure.png)

## Requirements

The code runs correctly with

* Python 3.6
* PyTorch 1.2
* Torchvision 0.4

## How to run

Update soon.

## Citation

If our code is helpful for your research, please cite our paper:

```
@inproceedings{Huang2021AGAM,
  author = {Huang, Siteng and Zhang, Min and Kang, Yachen and Wang, Donglin},
  title = {Attributes-Guided and Pure-Visual Attention Alignment for Few-Shot Recognition},
  booktitle = {The Thirty-Fifth AAAI Conference on Artificial Intelligence (AAAI 2021)},
  month = {February},
  year = {2021}
}
```

## Acknowledgement

Our code references the following projects:

* [Torchmeta](https://github.com/tristandeleu/pytorch-meta)
* [FEAT](https://github.com/Sha-Lab/FEAT)

